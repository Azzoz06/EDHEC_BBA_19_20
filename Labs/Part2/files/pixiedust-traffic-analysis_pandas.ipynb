{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Watson Studio with PixieDust\n",
    "\n",
    "### Analyze data and build a dashboard with notebooks, and PixieDust\n",
    "\n",
    "Interactive notebooks are powerful tools for fast and flexible experimentation and data analysis. Notebooks can contain live code, static text, equations and visualizations. \n",
    "\n",
    "In this lab, we will walk through how to use PixieDust with notebooks to:\n",
    "- Analyze open data around traffic accidents in San Francisco\n",
    "- Build charts and maps to discover insights\n",
    "\n",
    "We will then show how to:\n",
    "- Build a dashboard that drills down into specific areas\n",
    "- Combine multiple data sources like crime or speeding zones to extract even more insights  \n",
    "\n",
    "![pixiedust](https://developer.ibm.com/clouddataservices/wp-content/uploads/sites/85/2017/03/pixiedust200.png)\n",
    "\n",
    "Learn more about PixieDust [Here](https://www.ibm.com/analytics/us/en/watson-data-platform/pixiedust/).\n",
    "\n",
    "This notebook runs on Python 3.5.\n",
    "\n",
    "This Lab is based on the tutorial published with step by step instructions here: [https://www.slideshare.net/DTAIEB/pixie-dust-overview](https://www.slideshare.net/DTAIEB/pixie-dust-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n",
    "# not use this file except in compliance with the License. You may obtain\n",
    "# a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n",
    "# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n",
    "# License for the specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade bokeh==0.12.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Procure PixieDust <a class=\"anchor\" id=\"install\"></a>\n",
    "Note that **IBM Watson Studio** Jupyter environment comes with a version of pixiedust already installed.  \n",
    "\n",
    "The next cell could be uncommented to upgrade PixieDust to the latest level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell, we will just import the `pixiedust` module into the Python namespace.   \n",
    "You will notice that this triggers initialization code of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before, you can use the PixieDust library it must be imported into the notebook.\n",
    "# This notebook requires version 1.0.6\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import San Francisco Police incidents data into our notebook environment\n",
    "Source: [San Francisco Open Data](https://datasf.org/opendata)\n",
    "> You may want to take a moment to explore all the data available at this site\n",
    "\n",
    "We will use pixiedust to easily load CSV data from a URL into a Pandas DataFrame.   \n",
    "Note that this operation may take quite some time, the downloaded file being rather large (about 1/2 GB).\n",
    "You will notice we use the `%%time` so-called cell magic to measure the loading time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load data into a Pandas dataframe from its URL\n",
    "#dfIncidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/956q-2t7k/rows.csv?accessType=DOWNLOAD\")\n",
    "#dfIncidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/wg3w-h783/rows.csv?accessType=DOWNLOAD\")\n",
    "#Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv\n",
    "dfIncidents = pixiedust.sampleData(\"https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data file loaded has {0:n} lines for a in-memory size of {1:n} bytes\".format(len(dfIncidents),dfIncidents.memory_usage().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration\n",
    "\n",
    "After successfully importing PixieDust and loading the sample data, we can use the `display()` API to quickly browse through and visualize the data to obtain immediate insights.\n",
    "\n",
    "Run the cell with below, then apply the following steps in the interactive output which will be created below:\n",
    "\n",
    "1. Explore the schema and browse the data\n",
    "   * Select _DataFrame Table_ icon (leftmost that looks like a grid or table) in the display widget\n",
    "   * This yields a tabular view of the data. This view can be scrolled through\n",
    "2. Explore the data graphically to answer questions, e.g. In which police district do the most police incidents occur?\n",
    "   * Choose the _Chart_ icon in the display widget below, select `Pie Chart` type\n",
    "   * Open the Options and verify that the settings are `Keys=PdDistrict`, `Values=IncidntNum`, `Aggregation=Count`\n",
    "   * The resulting pie chart shows that `Southern`, `Mission` and `Northern` are the districts where there are most incidents.\n",
    "3. We can now dig one level deeper by clustering by how each accident was resolved:\n",
    "   * Choose again _Chart_ icon in the display widget and select `Bar Chart` type\n",
    "      * Note that you may get an error stating that `bokeh` library is back level, in this case switch the Renderer back to matplotlib (top right drop-down list)\n",
    "   * Open the options and check that `Keys = PdDistrict`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * On the right side, make sure that the setting is **`Cluster By: Resolution`**\n",
    "   * You may want to uncheck the ' show legend' box to fully view the bars.\n",
    "   * We notice there that `Southern`, `Northern`, `Mission` and `Central` have the most unresolved incidents, while `Southern` and `Mission` has a relatively higher arrest count.\n",
    "   * You can switch to `Type` : `Stacked` to compare cumulated incidents. \n",
    "4. we can also investigate on what day of the week do the most police incidents occur:\n",
    "   * Choose the _Chart_ icon in the display widget and select `Bar Chart`\n",
    "   * Change the Options (by drag&drop) so that `Keys = DayOfWeek`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Set `Cluster By` back to None to get overall figures, which shows that all days are very similar\n",
    "   * Since the height of the bars are so similar, use `Pie Chart - Options: Keys = DayOfWeek, Values = IncidntNum, Aggregation = Count)`, which confirms that each day has almost its equal share of 13-15% of the total.\n",
    "\n",
    "You may want to take a moment to explore the possibility of the Display API by watching this [video](https://www.youtube.com/watch?v=FoOHFlkCaXI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "chartsize": "99",
      "charttype": "stacked",
      "clusterby": "Resolution",
      "handlerId": "barChart",
      "keyFields": "PdDistrict",
      "legend": "false",
      "mpld3": "false",
      "rendererId": "bokeh",
      "rowCount": "1000",
      "sortby": "Values ASC",
      "valueFields": "IncidntNum"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(dfIncidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More data Exploration and Hypothesis\n",
    "As we have just seen, we can quickly identify a couple of areas of interest in our data without having to write a single line of code:\n",
    "\n",
    "1) Most incidents happen in the Southern  police district, and\n",
    "\n",
    "2) The number of incidents is approximately the same for each day, ranging from 13-15% of the total per day.\n",
    "\n",
    "When looking at the table view, we also realized that our data needs some cleansing if we want to make analysis easier. Specifically:\n",
    "\n",
    "- The `Time` field is a string, so we'll need to add an `Hour` column if we want to see the time of day when most incidents occur, and\n",
    "- The `DayOfWeek` values are rendered in alphabetical order by default instead of chronological order, so we should rename them to make it easier to see how the number of incidents changes over the course of the week, in a new `DoWIdx` columns\n",
    "- And we should condense the outcome types of each police incident if we want to see the most common resolutions of police incidents in each police district, since the clustering above was undifferentiating. We will create a calculated column `Res` \n",
    "\n",
    "Let's cleanse the data using Pandas DataFrame code and re-investigate before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the hour value of a time string, e.g. getHour(\"05:30\") will yield the integer value 5\n",
    "# Add Hour column and refine outcomes from police incidents using a lambda\n",
    "dfIncidents[\"Hour\"] = dfIncidents[\"Time\"].map(lambda timeStr: int(timeStr.split(':')[0]))\n",
    "dfIncidents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename weekdays to sorted names in order to enable mini time-series analysis\n",
    "DoWIndex={'Monday':'1-Mon','Tuesday':'2-Tues','Wednesday':'3-Wed','Thursday':'4-Thur','Friday':'5-Fri','Saturday':'6-Sat','Sunday':'7-Sun'}\n",
    "    \n",
    "dfIncidents['DoWIdx'] = dfIncidents['DayOfWeek'].map(lambda x: DoWIndex[x])\n",
    "    \n",
    "dfIncidents.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many tof each type of resolution we have\n",
    "dfIncidents['Resolution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coalesce incident resolutions in only 3 categories: `Arrest`, `No Resolution`  or `Other`\n",
    "dfIncidents[\"Res\"] = dfIncidents['Resolution'].map(lambda x: 'Arrest' if 'ARREST' in x else 'No Resolution' if x == 'NONE' else 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many types after coalescing\n",
    "dfRes=dfIncidents['Res'].value_counts().to_frame()\n",
    "dfRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are now ready for more data exploration\n",
    "\n",
    "Run the cell below which operates on the augmented dataframe\n",
    "\n",
    "1. Hypothesis: Do incidents in one police district result in more arrests than other police districts?\n",
    "    * To find out, run the cell below and set the following display options:\n",
    "    * Bar Chart\n",
    "    * _Options_: `Keys = PdDistrict`, `Values = IncidntNum`, `Aggregation = Count`, `Cluster By: Res`\n",
    "    * The districts where there are more arrests than no resolution stand out (`Richmnond`, `Mission`, ...)\n",
    "    I find the `horizontal` orientation better for showing the Police Districts.\n",
    "\n",
    "2. Question: How does the number of incidents change over the course of the week?\n",
    "    * To answer, change the options to \n",
    "    * Line Chart\n",
    "    * _Options: `Keys = DayOfWeek`, `Values = IncidntNum`, `Aggregation = Count`,  `Cluster By: None`\n",
    "    * We now see a slow increase to a slight peak on Fridays, with a decrease until Sundays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "chartsize": "90",
      "charttype": "grouped",
      "clusterby": "Res",
      "handlerId": "barChart",
      "keyFields": "PdDistrict",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "500",
      "valueFields": "IncidntNum"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(dfIncidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learned\n",
    "A few lines of code makes it a lot easier to see that:\n",
    "\n",
    "1) Incidents in the Mission and Southern police districts are much more likely to result in arrest than all other districts, and\n",
    "\n",
    "2) The number of incidents raises slightly during the middle of the week, through Friday, and then decreases through Sunday.\n",
    "\n",
    "## Now let's focus on the Mission police district using some filtering\n",
    "We create a new `dfMission` dataframe which has only Mission PdDistrict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMission = dfIncidents[dfIncidents['PdDistrict']=='MISSION']\n",
    "dfMission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to display a Mission-focused map\n",
    "We are now able to drill down deeper in the structure of `Mission` incidents\n",
    "1. Question: Where in Mission do most incidents happen?\n",
    "   * Select `Map` as type (we have x,y coordinates for incidents locations)\n",
    "   * Set Options to  `Keys = [X,Y]`, `Values = IncidntNum`\n",
    "   * Set the Renderer to `mapbox`, kind: `density-map`\n",
    "   * It appears that incidents are distributed with a predominance along the district's central street 'Mission St', with a predominance on main streets and avenues.\n",
    "\n",
    "2. Question: What time of day do most incidents occur?\n",
    "   * select type back to `Line Chart`\n",
    "   * Set Options: `Keys = Hour`, `Values = IncidntNum`, `Aggregation = Count`\n",
    "   * Two spikes of incidents clearly matches noon and evening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "COUNT",
      "basemap": "light-v9",
      "chartsize": "91",
      "handlerId": "lineChart",
      "keyFields": "Hour",
      "kind": "choropleth-cluster",
      "mapboxtoken": "pk.eyJ1IjoicmFqcnNpbmdoIiwiYSI6ImNqM2s4ZDg4djAwcGYyd3BwaGxwaDV3bWoifQ.d5Rklkdu5MeGAnXu1GMNYw",
      "numbins": "5",
      "rendererId": "mapbox",
      "rowCount": "1000",
      "timeseries": "false",
      "valueFields": "IncidntNum"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(dfMission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learned:\n",
    "Most of the results from looking at the incident times are unsurprising:\n",
    "\n",
    "- Number of incidents drop sharply very early morning (people probably sleeping),\n",
    "- Steady increase in number of incidents until noon,\n",
    "- Fairly high numbers from 3:00 PM until 8:00 PM,\n",
    "- Surprisingly, incidents decline after 8:00PM.\n",
    "\n",
    "The interesting thing here is the fact that the peaks are at noon and from 3:00PM until 8:00 PM, as one might expect the later evening times to be more problematic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the pixiedust lab. In the nextlab, we will se Watson Studio can be used to build dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
